{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit Recognizer.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-ht6vg44wn_i",
        "colab_type": "code",
        "outputId": "0ad25c56-bf66-4c5e-c204-fd8a32c24169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "import numpy as np \n",
        "from numpy import genfromtxt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0A8TYehPzS_f",
        "colab_type": "code",
        "outputId": "8f298e52-0d65-43e6-99c0-94c36a49c233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "njXTXhXBz2CJ",
        "colab_type": "code",
        "outputId": "9e2a2a0b-ad23-4167-fcb4-5e9d0d7d00b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/gdrive/My Drive/Datasets/Digit_Recognizer\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pvi4B3o23CKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_size = None\n",
        "\n",
        "def training_data(file_name):\n",
        "    raw_data = genfromtxt(file_name, delimiter=',', skip_header=1)\n",
        "\n",
        "    raw_sample_image = raw_data[0][1:]\n",
        "    image_size = int(np.sqrt(len(raw_sample_image)))\n",
        "    \n",
        "    print(image_size)\n",
        "    \n",
        "    X_shape = (len(raw_data), image_size, image_size, 1)\n",
        "    y_shape = (len(raw_data), 10)\n",
        "\n",
        "    X_data = np.zeros(X_shape)\n",
        "    y_data = np.zeros(y_shape)\n",
        "    \n",
        "    for index, datum in enumerate(raw_data):\n",
        "        X_data[index] = np.array(datum[1:]/255).reshape(image_size, image_size, 1)\n",
        "        y_data[index] = np_utils.to_categorical(int(datum[0]), 10)        \n",
        "\n",
        "    return X_data, y_data\n",
        "  \n",
        "def load_eval_data(file_name):\n",
        "    raw_data = np.genfromtxt(file_name, delimiter=',', skip_header=1)\n",
        "\n",
        "    raw_sample_image = raw_data[0]\n",
        "    image_size = int(np.sqrt(len(raw_sample_image)))\n",
        "    X_shape = (len(raw_data), image_size, image_size, 1)\n",
        "\n",
        "    X_data = np.zeros(X_shape)\n",
        "    for index, datum in enumerate(raw_data):\n",
        "        X_data[index] = np.array(datum/255).reshape(image_size, image_size, 1)\n",
        "\n",
        "    return X_data\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDC2fW8o3FDO",
        "colab_type": "code",
        "outputId": "bbf7ac8b-9352-4a5a-a336-6f32cff43591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2655
        }
      },
      "cell_type": "code",
      "source": [
        "train_file = '/content/gdrive/My Drive/Datasets/Digit_Recognizer/train.csv'\n",
        "test_file = '/content/gdrive/My Drive/Datasets/Digit_Recognizer/test.csv'\n",
        "\n",
        "print(\"Loading training data\")\n",
        "X_train, y_train = training_data(train_file)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(image_size, image_size, 1)))\n",
        "model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=model_weights_file, verbose=1, save_best_only=True)\n",
        "stopper = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='auto')\n",
        "hist = model.fit(X_train, y_train, batch_size=32, epochs=1000, validation_split=0.2, callbacks=[checkpointer, stopper], verbose=1, shuffle=True)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training data\n",
            "28\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, None, None, 16)    160       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, None, None, 16)    2320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, None, 16)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, None, None, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, None, None, 32)    4640      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, None, None, 32)    9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, None, None, 64)    18496     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               32500     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 109,302\n",
            "Trainable params: 109,302\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 33600 samples, validate on 8400 samples\n",
            "Epoch 1/1000\n",
            "33600/33600 [==============================] - 18s 528us/step - loss: 0.7906 - acc: 0.7249 - val_loss: 0.2160 - val_acc: 0.9331\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.21605, saving model to /content/gdrive/My Drive/Datasets/Digit_Recognizer/digitrecognizer.model.best.hdf5\n",
            "Epoch 2/1000\n",
            "33600/33600 [==============================] - 13s 391us/step - loss: 0.1749 - acc: 0.9490 - val_loss: 0.0905 - val_acc: 0.9730\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.21605 to 0.09047, saving model to /content/gdrive/My Drive/Datasets/Digit_Recognizer/digitrecognizer.model.best.hdf5\n",
            "Epoch 3/1000\n",
            "33600/33600 [==============================] - 13s 390us/step - loss: 0.1104 - acc: 0.9681 - val_loss: 0.0623 - val_acc: 0.9813\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.09047 to 0.06231, saving model to /content/gdrive/My Drive/Datasets/Digit_Recognizer/digitrecognizer.model.best.hdf5\n",
            "Epoch 4/1000\n",
            "33600/33600 [==============================] - 13s 390us/step - loss: 0.0827 - acc: 0.9757 - val_loss: 0.0451 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.06231 to 0.04511, saving model to /content/gdrive/My Drive/Datasets/Digit_Recognizer/digitrecognizer.model.best.hdf5\n",
            "Epoch 5/1000\n",
            "33600/33600 [==============================] - 13s 391us/step - loss: 0.0683 - acc: 0.9798 - val_loss: 0.0365 - val_acc: 0.9875\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.04511 to 0.03646, saving model to /content/gdrive/My Drive/Datasets/Digit_Recognizer/digitrecognizer.model.best.hdf5\n",
            "Epoch 6/1000\n",
            "33600/33600 [==============================] - 13s 390us/step - loss: 0.0609 - acc: 0.9821 - val_loss: 0.0377 - val_acc: 0.9880\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03646\n",
            "Epoch 7/1000\n",
            "33600/33600 [==============================] - 13s 389us/step - loss: 0.0539 - acc: 0.9843 - val_loss: 0.0499 - val_acc: 0.9845\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.03646\n",
            "Epoch 8/1000\n",
            "33600/33600 [==============================] - 13s 388us/step - loss: 0.0472 - acc: 0.9865 - val_loss: 0.0508 - val_acc: 0.9846\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.03646\n",
            "Epoch 9/1000\n",
            "33600/33600 [==============================] - 13s 383us/step - loss: 0.0460 - acc: 0.9869 - val_loss: 0.0371 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.03646\n",
            "Epoch 10/1000\n",
            "33600/33600 [==============================] - 13s 386us/step - loss: 0.0433 - acc: 0.9879 - val_loss: 0.0375 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.03646\n",
            "Epoch 11/1000\n",
            "33600/33600 [==============================] - 13s 384us/step - loss: 0.0434 - acc: 0.9893 - val_loss: 0.0385 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.03646\n",
            "Epoch 12/1000\n",
            "33600/33600 [==============================] - 13s 384us/step - loss: 0.0389 - acc: 0.9895 - val_loss: 0.0369 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.03646\n",
            "Epoch 13/1000\n",
            "33600/33600 [==============================] - 13s 384us/step - loss: 0.0395 - acc: 0.9889 - val_loss: 0.0353 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.03646 to 0.03530, saving model to /content/gdrive/My Drive/Datasets/Digit_Recognizer/digitrecognizer.model.best.hdf5\n",
            "Epoch 14/1000\n",
            "33600/33600 [==============================] - 13s 383us/step - loss: 0.0387 - acc: 0.9905 - val_loss: 0.0454 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.03530\n",
            "Epoch 15/1000\n",
            "33600/33600 [==============================] - 13s 385us/step - loss: 0.0359 - acc: 0.9904 - val_loss: 0.0380 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.03530\n",
            "Epoch 16/1000\n",
            "33600/33600 [==============================] - 13s 384us/step - loss: 0.0360 - acc: 0.9901 - val_loss: 0.0306 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.03530 to 0.03065, saving model to /content/gdrive/My Drive/Datasets/Digit_Recognizer/digitrecognizer.model.best.hdf5\n",
            "Epoch 17/1000\n",
            "33600/33600 [==============================] - 13s 385us/step - loss: 0.0369 - acc: 0.9905 - val_loss: 0.0362 - val_acc: 0.9899\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.03065\n",
            "Epoch 18/1000\n",
            "33600/33600 [==============================] - 13s 384us/step - loss: 0.0380 - acc: 0.9908 - val_loss: 0.0335 - val_acc: 0.9917\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.03065\n",
            "Epoch 19/1000\n",
            "33600/33600 [==============================] - 13s 383us/step - loss: 0.0367 - acc: 0.9912 - val_loss: 0.0368 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.03065\n",
            "Epoch 20/1000\n",
            "33600/33600 [==============================] - 13s 385us/step - loss: 0.0360 - acc: 0.9910 - val_loss: 0.0390 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.03065\n",
            "Epoch 21/1000\n",
            "33600/33600 [==============================] - 13s 384us/step - loss: 0.0347 - acc: 0.9910 - val_loss: 0.0436 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.03065\n",
            "Epoch 22/1000\n",
            "33600/33600 [==============================] - 13s 387us/step - loss: 0.0367 - acc: 0.9914 - val_loss: 0.0689 - val_acc: 0.9883\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.03065\n",
            "Epoch 23/1000\n",
            "33600/33600 [==============================] - 13s 385us/step - loss: 0.0370 - acc: 0.9906 - val_loss: 0.0487 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.03065\n",
            "Epoch 24/1000\n",
            "33600/33600 [==============================] - 13s 386us/step - loss: 0.0379 - acc: 0.9913 - val_loss: 0.0485 - val_acc: 0.9911\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.03065\n",
            "Epoch 25/1000\n",
            "33600/33600 [==============================] - 13s 387us/step - loss: 0.0352 - acc: 0.9920 - val_loss: 0.0472 - val_acc: 0.9904\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.03065\n",
            "Epoch 26/1000\n",
            "33600/33600 [==============================] - 13s 388us/step - loss: 0.0396 - acc: 0.9912 - val_loss: 0.0474 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.03065\n",
            "Epoch 00026: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QR_uWUux3TRc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_file = '/content/gdrive/My Drive/Datasets/Digit_Recognizer/submission.csv'\n",
        "X_eval = load_eval_data(test_file)\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    f.write('ImageId,Label\\n')\n",
        "    y_eval = model.predict(X_eval)\n",
        "    for index, y_hat in enumerate(y_eval):\n",
        "        prediction = np.argmax(y_hat)\n",
        "        f.write(str(index+1)+','+str(prediction)+'\\n')\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}